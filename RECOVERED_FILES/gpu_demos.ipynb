{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU usage etiquette on Tufts HPC\n",
        "\n",
        "- Request only what you’ll actively use. Avoid rapid-fire short `srun` calls that thrash the scheduler.\n",
        "- Prefer an interactive allocation sized for your exploration window (e.g., 1–4 GPUs for 1–3 hours), then iterate inside that shell.\n",
        "- Release GPUs when idle. Don’t “park” large-GPU nodes while not running.\n",
        "- For exploratory or spiky workloads, consider `preempt` partition (jobs may be preempted but start faster).\n",
        "- Keep HOME clean; place all code, caches, envs, and models under `/cluster/tufts/datalab/zwu09` or class storage.\n",
        "- Monitor your jobs: `squeue -u $USER`. Inspect partitions: `sinfo -o \"%P %a %l %D %c %m %G\"`.\n",
        "\n",
        "Example interactive sessions:\n",
        "```bash\n",
        "# 2 GPUs (A100) for 2 hours\n",
        "srun -p gpu --gres=gpu:a100:2 -t 02:00:00 -n 1 -c 16 --pty bash\n",
        "\n",
        "# 4 GPUs (A100) for 2 hours\n",
        "srun -p gpu --gres=gpu:a100:4 -t 02:00:00 -n 1 -c 32 --pty bash\n",
        "\n",
        "# 2 GPUs (H100) on preempt (may queue/preempt)\n",
        "srun -p preempt --gres=gpu:h100:2 -t 02:00:00 -n 1 -c 32 --pty bash\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Kill junk\n",
        "scancel 15654372 15654410\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System checks (run inside an interactive shell)\n",
        "!nvidia-smi -L || true\n",
        "!python -c \"import torch, sys; print('torch:', torch.__version__, 'cuda:', torch.version.cuda, 'is_cuda_available:', torch.cuda.is_available()); print(sys.version)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem                       Size  Used Avail Use% Mounted on\n",
            "10.246.194.88:/projects/datalab  2.3T  2.3T  6.7G 100% /cluster/tufts/datalab\n",
            "Filesystem                          Size  Used Avail Use% Mounted on\n",
            "10.246.194.84:/projects/em212class 1000G  8.9G  991G   1% /cluster/tufts/em212class\n",
            "PARTITION AVAIL TIMELIMIT NODES CPUS MEMORY GRES\n",
            "interactive up 4:00:00 2 36 248000 (null)\n",
            "batch* up 7-00:00:00 71 36+ 120000+ (null)\n",
            "mpi up 7-00:00:00 70 36+ 120000+ (null)\n",
            "gpu up 7-00:00:00 1 64 190000 gpu:a100:2\n",
            "gpu up 7-00:00:00 10 64+ 756121+ gpu:a100:8\n",
            "gpu up 7-00:00:00 1 72 248000 gpu:p100:4\n",
            "largemem up 7-00:00:00 2 36+ 1000000 (null)\n",
            "preempt up 7-00:00:00 3 64+ 190000+ gpu:a100:2\n",
            "preempt up 7-00:00:00 9 64+ 756121+ gpu:a100:8\n",
            "preempt up 7-00:00:00 97 36+ 120000+ (null)\n",
            "preempt up 7-00:00:00 9 128 248000+ gpu:l40:4\n",
            "preempt up 7-00:00:00 1 64 368000 gpu:v100:3\n",
            "preempt up 7-00:00:00 2 64+ 256000+ gpu:v100:4\n",
            "preempt up 7-00:00:00 1 72 248000 gpu:p100:4\n",
            "preempt up 7-00:00:00 1 72 256000 gpu:p100:6\n",
            "preempt up 7-00:00:00 1 64 168000 gpu:v100:2\n",
            "preempt up 7-00:00:00 1 64 184000 gpu:rtx_6000:2\n",
            "preempt up 7-00:00:00 20 80 92000 gpu:t4:4\n",
            "preempt up 7-00:00:00 1 128 1030000 gpu:rtx_6000ada:4\n",
            "preempt up 7-00:00:00 1 224 2060000 gpu:h100:3\n",
            "preempt up 7-00:00:00 1 128 1020000 gpu:l40s:4\n",
            "preempt up 7-00:00:00 4 128 510000 gpu:rtx_a5000:8\n",
            "preempt up 7-00:00:00 1 96 1030000 gpu:rtx_a6000:8\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "          15654372   preempt     bash    zwu09 PD       0:00      1 (Resources)\n",
            "          15654410   preempt     bash    zwu09 PD       0:00      1 (Priority)\n",
            "          15653447   preempt sys/dash    zwu09  R    2:39:17      1 s1cmp010\n"
          ]
        }
      ],
      "source": [
        "# Storage and cluster discovery (run on login or interactive node)\n",
        "!df -h /cluster/tufts/datalab\n",
        "!df -h /cluster/tufts/em212class\n",
        "!sinfo -o \"%P %a %l %D %c %m %G\" | sed -n '1,40p'\n",
        "!squeue -u $USER | sed -n '1,30p'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime configuration for caches and model choice\n",
        "import os\n",
        "os.environ['HF_HOME'] = '/cluster/tufts/datalab/zwu09/caches/huggingface'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/cluster/tufts/datalab/zwu09/caches/huggingface'\n",
        "os.environ['PIP_CACHE_DIR'] = '/cluster/tufts/datalab/zwu09/caches/pip'\n",
        "os.environ['TORCH_HOME'] = '/cluster/tufts/datalab/zwu09/caches/torch'\n",
        "os.environ['TMPDIR'] = '/cluster/tufts/datalab/zwu09/tmp'\n",
        "\n",
        "# Use a lightweight, fast, and memory-efficient SDXL variant for testing\n",
        "model_id = os.environ.get('SD_MODEL_ID', 'stabilityai/sdxl-turbo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in /cluster/tufts/hpc/tools/miniforge3/24.7.1/lib/python3.12/site-packages (24.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.12 are installed in '/cluster/home/zwu09/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed pip-25.2\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.12/site-packages (2.2.2+cu121)\n",
            "Requirement already satisfied: torchvision in ./.local/lib/python3.12/site-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in ./.local/lib/python3.12/site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /cluster/tufts/hpc/tools/miniforge3/24.7.1/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in ./.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /cluster/tufts/hpc/tools/miniforge3/24.7.1/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.12/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.12/site-packages (from torchvision) (10.2.0)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.1.0 in ./.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /cluster/tufts/hpc/tools/miniforge3/24.7.1/lib/python3.12/site-packages (from torch) (73.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /cluster/tufts/hpc/tools/miniforge3/24.7.1/lib/python3.12/site-packages (from jinja2->torch) (3.0.1)\n",
            "Installing collected packages: nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.19.3[32m0/3\u001b[0m [nvidia-nccl-cu12]\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [nvidia-nccl-cu12]\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 8.9.2.260m [nvidia-nccl-cu12]\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\u001b[0m \u001b[32m0/3\u001b[0m [nvidia-nccl-cu12]\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [nvidia-cudnn-cu12]\n",
            "\u001b[2K  Attempting uninstall: torchm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [nvidia-cudnn-cu12]\n",
            "\u001b[2K    Found existing installation: torch 2.2.2+cu121━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [nvidia-cudnn-cu12]\n",
            "\u001b[2K    Uninstalling torch-2.2.2+cu121:[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [torch]nn-cu12]\n",
            "\u001b[2K      Successfully uninstalled torch-2.2.2+cu121\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [torch]\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [torch]\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2, torchfrtrace and torchrun are installed in '/cluster/home/zwu09/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torch]32m2/3\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2KSuccessfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.21.5 torch-2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Installer (inside kernel) — A100-friendly stack\n",
        "!{sys.executable} -m pip install --upgrade pip\n",
        "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!{sys.executable} -m pip install \"diffusers[torch]\" transformers accelerate safetensors bitsandbytes einops --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/cluster/home/zwu09/.local/lib/python3.12/site-packages/huggingface_hub/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Diffusers demo (Stable Diffusion XL) — adjust model if needed\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionXLPipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-xl-base-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/diffusers/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.27.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     DIFFUSERS_SLOW_IMPORT,\n\u001b[1;32m      7\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m      8\u001b[0m     _LazyModule,\n\u001b[1;32m      9\u001b[0m     is_flax_available,\n\u001b[1;32m     10\u001b[0m     is_k_diffusion_available,\n\u001b[1;32m     11\u001b[0m     is_librosa_available,\n\u001b[1;32m     12\u001b[0m     is_note_seq_available,\n\u001b[1;32m     13\u001b[0m     is_onnx_available,\n\u001b[1;32m     14\u001b[0m     is_scipy_available,\n\u001b[1;32m     15\u001b[0m     is_torch_available,\n\u001b[1;32m     16\u001b[0m     is_torchsde_available,\n\u001b[1;32m     17\u001b[0m     is_transformers_available,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     ],\n\u001b[1;32m     51\u001b[0m }\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/diffusers/utils/__init__.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replace_example_docstring\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_modules_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_to_gif, export_to_obj, export_to_ply, export_to_video\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     PushToHubMixin,\n\u001b[1;32m     42\u001b[0m     _add_variant,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     http_user_agent,\n\u001b[1;32m     46\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/diffusers/utils/dynamic_modules_utils.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m request\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_download, hf_hub_download, model_info\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_hf_hub_args\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/cluster/home/zwu09/.local/lib/python3.12/site-packages/huggingface_hub/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Diffusers demo (Stable Diffusion XL) — adjust model if needed\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A photorealistic portrait of a person wearing futuristic sunglasses, studio lighting\"\n",
        "image = pipe(prompt, guidance_scale=7.0, num_inference_steps=20).images[0]\n",
        "image.save(\"sdxl_demo.png\")\n",
        "image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Honor external model selection via env var\n",
        "import os\n",
        "SD_MODEL_ID = os.environ.get('SD_MODEL_ID', 'stabilityai/sdxl-turbo')\n",
        "print('Using model:', SD_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small torch smoke test (device)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'cuda available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TinyLlama demo (small LLM that fits on 1 GPU)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "prompt = \"Write a 3-line explanation of why GPUs accelerate deep learning.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
        "outputs = model.generate(**inputs, max_new_tokens=128)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "allenML2",
      "language": "python",
      "name": "allen2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
