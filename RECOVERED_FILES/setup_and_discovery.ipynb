{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tufts HPC setup and discovery\n",
        "\n",
        "Goals:\n",
        "- Keep HOME clean; work under `/cluster/tufts/datalab/zwu09` (and class path)\n",
        "- Inspect storage and cluster resources\n",
        "- Create a Python env + Jupyter kernel\n",
        "- Launch a tunneled Jupyter server on a compute node\n",
        "\n",
        "## Current Session Status\n",
        "- **Job ID**: 15686311\n",
        "- **Node**: s1cmp005.pax.tufts.edu\n",
        "- **Resources**: 2x A100 80GB GPUs, 40 CPUs, 40GB RAM\n",
        "- **Jupyter Server**: Running on port 8891 with token \"allen\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Kernel Connection Test ===\n",
            "Python version: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:36:51) [GCC 12.4.0]\n",
            "Working directory: /cluster/tufts/datalab/zwu09\n",
            "User: zwu09\n",
            "Node: login-prod-01.pax.tufts.edu\n",
            "Job ID: 15686311\n",
            "\n",
            "=== GPU Resources ===\n",
            "CUDA available: True\n",
            "GPU count: 2\n",
            "GPU 0: NVIDIA A100 80GB PCIe\n",
            "  Memory: 79.2 GB\n",
            "GPU 1: NVIDIA A100 80GB PCIe\n",
            "  Memory: 79.2 GB\n",
            "\n",
            "=== CPU and Memory ===\n",
            "CPU count: 128\n",
            "Memory: 1007.3 GB\n"
          ]
        }
      ],
      "source": [
        "# Test kernel connection and check current session resources\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(\"=== Kernel Connection Test ===\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"User: {os.getenv('USER', 'unknown')}\")\n",
        "print(f\"Node: {os.getenv('HOSTNAME', 'unknown')}\")\n",
        "print(f\"Job ID: {os.getenv('SLURM_JOB_ID', 'unknown')}\")\n",
        "print()\n",
        "\n",
        "print(\"=== GPU Resources ===\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not available\")\n",
        "print()\n",
        "\n",
        "print(\"=== CPU and Memory ===\")\n",
        "print(f\"CPU count: {os.cpu_count()}\")\n",
        "print(f\"Memory: {os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**3):.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Storage and cluster discovery - keeping HOME clean\n",
        "import subprocess\n",
        "\n",
        "print(\"=== Storage Information ===\")\n",
        "print(\"Datalab storage:\")\n",
        "!df -h /cluster/tufts/datalab\n",
        "print(\"\\nClass storage:\")\n",
        "!df -h /cluster/tufts/em212class\n",
        "print(\"\\nHome directory usage (should be minimal):\")\n",
        "!du -h --max-depth=1 ~ | sort -hr | head -n 10\n",
        "print()\n",
        "\n",
        "print(\"=== Current Job Status ===\")\n",
        "!squeue -u $USER\n",
        "print()\n",
        "\n",
        "print(\"=== Cluster Partitions ===\")\n",
        "!sinfo -o \"%P %a %l %D %c %m %G\" | sed -n '1,20p'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test GPU functionality with both A100s\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"=== GPU Functionality Test ===\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Testing {torch.cuda.device_count()} GPUs...\")\n",
        "    \n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"\\n--- GPU {i} Test ---\")\n",
        "        torch.cuda.set_device(i)\n",
        "        \n",
        "        # Create a test tensor\n",
        "        device = torch.device(f'cuda:{i}')\n",
        "        x = torch.randn(1000, 1000, device=device)\n",
        "        y = torch.randn(1000, 1000, device=device)\n",
        "        \n",
        "        # Test computation\n",
        "        start_time = time.time()\n",
        "        z = torch.matmul(x, y)\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        \n",
        "        print(f\"Matrix multiplication on GPU {i}: {end_time - start_time:.4f} seconds\")\n",
        "        print(f\"Result shape: {z.shape}\")\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.1f} MB\")\n",
        "        print(f\"GPU memory cached: {torch.cuda.memory_reserved(i) / 1024**2:.1f} MB\")\n",
        "        \n",
        "        # Clear memory\n",
        "        del x, y, z\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    print(\"\\n=== Multi-GPU Test ===\")\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        # Test data transfer between GPUs\n",
        "        x = torch.randn(100, 100, device='cuda:0')\n",
        "        y = x.to('cuda:1')\n",
        "        print(f\"Data transfer from GPU 0 to GPU 1: {x.shape} -> {y.shape}\")\n",
        "        print(\"Multi-GPU setup working correctly!\")\n",
        "    else:\n",
        "        print(\"Only one GPU available\")\n",
        "        \n",
        "else:\n",
        "    print(\"CUDA not available - check your setup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System Check with HPC Commands\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_cmd(cmd, desc):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {desc}\")\n",
        "    try:\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(f\"STDERR: {result.stderr}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    print()\n",
        "\n",
        "# Check system using proper HPC commands\n",
        "run_cmd(\"df -h /cluster/tufts/datalab\", \"Disk space\")\n",
        "run_cmd(\"squeue -u $USER\", \"My jobs\") \n",
        "run_cmd(\"nvidia-smi\", \"GPU status\")\n",
        "run_cmd(\"pip list | grep -E '(torch|diffusers|transformers)'\", \"Key packages\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Diffusion Model Test (with auto-install)\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=== Simple Diffusion Model Test ===\")\n",
        "\n",
        "# Set up cache directories\n",
        "os.environ['HF_HOME'] = '/cluster/tufts/datalab/zwu09/caches/huggingface'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/cluster/tufts/datalab/zwu09/caches/huggingface'\n",
        "os.environ['TORCH_HOME'] = '/cluster/tufts/datalab/zwu09/caches/torch'\n",
        "\n",
        "cache_dirs = [\n",
        "    '/cluster/tufts/datalab/zwu09/caches/huggingface',\n",
        "    '/cluster/tufts/datalab/zwu09/caches/torch'\n",
        "]\n",
        "for cache_dir in cache_dirs:\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Cache directories set up: {cache_dirs}\")\n",
        "\n",
        "# Check and install required packages\n",
        "required_packages = ['diffusers', 'transformers', 'accelerate', 'pillow']\n",
        "missing_packages = []\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"✅ {package} available\")\n",
        "    except ImportError:\n",
        "        missing_packages.append(package)\n",
        "        print(f\"❌ {package} not available\")\n",
        "\n",
        "if missing_packages:\n",
        "    print(f\"\\nInstalling missing packages: {missing_packages}\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
        "        print(\"✅ Packages installed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to install packages: {e}\")\n",
        "        print(\"Continuing with basic GPU tests...\")\n",
        "\n",
        "# Try to run a simple diffusion test\n",
        "try:\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image\n",
        "    \n",
        "    print(\"\\n=== Running Diffusion Model Test ===\")\n",
        "    print(\"Loading Stable Diffusion model...\")\n",
        "    \n",
        "    # Use a smaller model for testing\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    \n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        cache_dir='/cluster/tufts/datalab/zwu09/caches/huggingface'\n",
        "    )\n",
        "    \n",
        "    pipe = pipe.to(\"cuda:0\")\n",
        "    pipe.enable_attention_slicing()\n",
        "    \n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "    \n",
        "    # Generate a simple image\n",
        "    prompt = \"a simple red circle on white background\"\n",
        "    print(f\"Generating image with prompt: '{prompt}'\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        num_inference_steps=10,  # Very fast for testing\n",
        "        guidance_scale=7.5,\n",
        "        height=256,\n",
        "        width=256\n",
        "    ).images[0]\n",
        "    \n",
        "    generation_time = time.time() - start_time\n",
        "    print(f\"✅ Image generated in {generation_time:.2f} seconds\")\n",
        "    \n",
        "    # Save image\n",
        "    output_path = \"/cluster/tufts/datalab/zwu09/simple_diffusion_test.png\"\n",
        "    image.save(output_path)\n",
        "    print(f\"Image saved to: {output_path}\")\n",
        "    \n",
        "    # Display image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Generated in {generation_time:.2f}s on A100\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"✅ Diffusion model test completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Diffusion test failed: {e}\")\n",
        "    print(\"Running fallback GPU performance test...\")\n",
        "    \n",
        "    # Fallback: Advanced GPU test\n",
        "    print(\"\\n=== Fallback: Advanced GPU Test ===\")\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    \n",
        "    # Test large matrix operations\n",
        "    sizes = [1000, 2000, 4000]\n",
        "    for size in sizes:\n",
        "        try:\n",
        "            print(f\"Testing {size}x{size} matrices...\")\n",
        "            x = torch.randn(size, size, device=device, dtype=torch.float16)\n",
        "            y = torch.randn(size, size, device=device, dtype=torch.float16)\n",
        "            \n",
        "            start = time.time()\n",
        "            z = torch.matmul(x, y)\n",
        "            torch.cuda.synchronize()\n",
        "            end = time.time()\n",
        "            \n",
        "            print(f\"  Matrix multiplication: {end-start:.4f} seconds\")\n",
        "            print(f\"  Memory used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "            \n",
        "            del x, y, z\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        except RuntimeError as e:\n",
        "            print(f\"  ❌ Failed at size {size}: {e}\")\n",
        "            break\n",
        "    \n",
        "    print(\"✅ Fallback GPU test completed!\")\n",
        "\n",
        "# Multi-GPU test\n",
        "print(\"\\n=== Multi-GPU Test ===\")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Testing {torch.cuda.device_count()} GPUs...\")\n",
        "    \n",
        "    device0 = torch.device(\"cuda:0\")\n",
        "    device1 = torch.device(\"cuda:1\")\n",
        "    \n",
        "    # Test data transfer\n",
        "    x0 = torch.randn(1000, 1000, device=device0, dtype=torch.float16)\n",
        "    start = time.time()\n",
        "    x0_to_1 = x0.to(device1)\n",
        "    torch.cuda.synchronize()\n",
        "    transfer_time = time.time() - start\n",
        "    print(f\"GPU 0 → GPU 1 transfer: {transfer_time:.4f} seconds\")\n",
        "    \n",
        "    # Test parallel computation\n",
        "    x1 = torch.randn(1000, 1000, device=device1, dtype=torch.float16)\n",
        "    start = time.time()\n",
        "    result0 = torch.matmul(x0, x0)\n",
        "    result1 = torch.matmul(x1, x1)\n",
        "    torch.cuda.synchronize()\n",
        "    parallel_time = time.time() - start\n",
        "    print(f\"Parallel computation: {parallel_time:.4f} seconds\")\n",
        "    \n",
        "    print(\"✅ Multi-GPU operations working correctly!\")\n",
        "else:\n",
        "    print(\"Only one GPU detected\")\n",
        "\n",
        "# Clean up\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n✅ All tests completed! Your A100 GPUs are ready for high-performance computing!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create env + kernel under datalab (one-time)\n",
        "!python3 -m venv /cluster/tufts/datalab/zwu09/envs/hoc\n",
        "!/cluster/tufts/datalab/zwu09/envs/hoc/bin/pip install --upgrade pip ipykernel jupyterlab\n",
        "!/cluster/tufts/datalab/zwu09/envs/hoc/bin/python -m ipykernel install --user --name hoc --display-name \"Tufts HPC (hoc)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Current Session Setup (COMPLETED ✅)\n",
        "\n",
        "### Resources Allocated:\n",
        "- **2x A100 80GB GPUs** (Job ID: 15686311)\n",
        "- **40 CPUs, 40GB RAM**\n",
        "- **Node**: s1cmp005.pax.tufts.edu\n",
        "\n",
        "### Jupyter Server Running:\n",
        "```bash\n",
        "# Already running on compute node:\n",
        "jupyter lab --no-browser --ip 127.0.0.1 --port 8891 --ServerApp.token=allen \\\n",
        "  --NotebookApp.notebook_dir=/cluster/tufts/datalab/zwu09\n",
        "```\n",
        "\n",
        "### SSH Tunnel Established:\n",
        "```bash\n",
        "# Already connected from laptop:\n",
        "ssh -J zwu09@login.pax.tufts.edu -L 8891:127.0.0.1:8891 zwu09@s1cmp005.pax.tufts.edu\n",
        "```\n",
        "\n",
        "### Cursor Connection:\n",
        "- **Jupyter server URL**: `http://localhost:8891/?token=allen`\n",
        "- **Kernel**: Tufts HPC (hoc) or allenML2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: set caches/temp under datalab for this session\n",
        "export HF_HOME=/cluster/tufts/datalab/zwu09/caches/huggingface\n",
        "export TRANSFORMERS_CACHE=/cluster/tufts/datalab/zwu09/caches/huggingface\n",
        "export PIP_CACHE_DIR=/cluster/tufts/datalab/zwu09/caches/pip\n",
        "export TORCH_HOME=/cluster/tufts/datalab/zwu09/caches/torch\n",
        "export TMPDIR=/cluster/tufts/datalab/zwu09/tmp\n",
        "mkdir -p \"$HF_HOME\" \"$TRANSFORMERS_CACHE\" \"$PIP_CACHE_DIR\" \"$TORCH_HOME\" \"$TMPDIR\"\n",
        "\n",
        "echo \"HF_HOME=$HF_HOME\"\n",
        "echo \"TRANSFORMERS_CACHE=$TRANSFORMERS_CACHE\"\n",
        "echo \"PIP_CACHE_DIR=$PIP_CACHE_DIR\"\n",
        "echo \"TORCH_HOME=$TORCH_HOME\"\n",
        "echo \"TMPDIR=$TMPDIR\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "10+10"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "allenML2",
      "language": "python",
      "name": "allen2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
