# LM Studio Server Updates Summary

**Date**: September 30, 2025  
**Status**: ‚úÖ Qwen2.5-Coder-7B tested and working!

---

## üéâ What's New

### 1. ‚úÖ **Model Caching** (No Re-downloads!)
- Models are now cached in `/cluster/tufts/datalab/zwu09/caches/huggingface`
- If you load a model once, it won't re-download next time
- Saves time and bandwidth!

### 2. üîÑ **Force Re-download Button**
- New "üîÑ Force Re-download" button
- Use this if a model is corrupted or you want latest version
- Regular "Load Model" button uses cache

### 3. üî• **Larger Model Recommendations**
Now showing **32B models** (best quality for H100):

#### LARGE Models (32B - Best Quality)
- `Qwen/Qwen2.5-Coder-32B-Instruct` - BEST coding model (~64GB)
- `Qwen/Qwen2.5-32B-Instruct` - BEST chat model (~64GB)
- `deepseek-ai/deepseek-coder-33b-instruct` - Advanced code (~66GB)

#### MEDIUM Models (7B-14B - Fast & Good)
- `Qwen/Qwen2.5-Coder-7B-Instruct` - ‚úÖ TESTED & WORKING
- `Qwen/Qwen2.5-14B-Instruct` - Better than 7B, faster than 32B
- `microsoft/Phi-3-medium-4k-instruct` - Excellent reasoning
- `mistralai/Mistral-7B-Instruct-v0.3` - Fast and reliable

### 4. üìà **Improved Limits**
- Max input: 2048 tokens (was 512)
- Max output: 512 tokens (was 100)
- Better for longer conversations!

---

## üì§ How to Upload Updated Server

### From Your Windows Laptop (Git Bash):

```bash
# Navigate to project directory
cd C:/Users/sdtyu/Documents/CursorRoot

# Upload to HPC
scp HPC/lm_studio_server_v3_improved.py zwu09@s1cmp010.pax.tufts.edu:/cluster/tufts/datalab/zwu09/
```

### Then on HPC (restart server):

```bash
# SSH to compute node
ssh zwu09@s1cmp010.pax.tufts.edu

# Stop old server
ps aux | grep lm_studio_server | grep -v grep | awk '{print $2}' | xargs kill

# Start new server
cd /cluster/tufts/datalab/zwu09
source /cluster/tufts/hpc/tools/anaconda/202307/etc/profile.d/conda.sh
conda activate lmstudio_v3
export HF_TOKEN=YOUR_HF_TOKEN_HERE
export PYTHONNOUSERSITE=1
export HF_HOME=/cluster/tufts/datalab/zwu09/caches/huggingface
export TRANSFORMERS_CACHE=/cluster/tufts/datalab/zwu09/caches/huggingface
nohup python lm_studio_server_v3_improved.py > lm_studio.log 2>&1 &

# Check it started
tail -30 lm_studio.log
```

---

## üöÄ Next Models to Try

### For Best Coding (32B):
```
Qwen/Qwen2.5-Coder-32B-Instruct
```
This is the **state-of-art** coding model that fits on H100!

### For Best General Chat (32B):
```
Qwen/Qwen2.5-32B-Instruct
```
Best multilingual chat model for H100.

### For Faster Alternative (14B):
```
Qwen/Qwen2.5-14B-Instruct
```
Sweet spot between speed and quality.

---

## üí° Model Caching Explained

### How It Works:
1. **First time loading** a model: Downloads from HuggingFace (~5-10 min)
2. **Second time loading** same model: Loads from cache (~1-2 min)
3. **Force Re-download**: Re-downloads fresh copy from HuggingFace

### Cache Location:
```
/cluster/tufts/datalab/zwu09/caches/huggingface/
```

### To Check Cache:
```bash
du -sh /cluster/tufts/datalab/zwu09/caches/huggingface/
```

### To Clear Cache (if running out of space):
```bash
rm -rf /cluster/tufts/datalab/zwu09/caches/huggingface/*
```

---

## üéØ Button Guide

### "Load Model (Use Cache)" ‚Üê Default
- Loads from cache if model was downloaded before
- Fast for models you've used
- Recommended for normal use

### "üîÑ Force Re-download" ‚Üê Special
- Always downloads fresh from HuggingFace
- Use if:
  - Model seems corrupted
  - Want latest version
  - Model cache is incomplete

---

## üìä Storage Planning

### Model Sizes (approximate):
- 7B model: ~14GB
- 14B model: ~28GB
- 32B model: ~64GB

### Your HPC Storage:
- Total: 400GB
- Can fit ~6 models (32B) or ~28 models (7B)
- Models persist between sessions!

---

## ‚úÖ Success Story

**Qwen/Qwen2.5-Coder-7B-Instruct** is loaded and working!

Next recommended models:
1. Try **Qwen/Qwen2.5-32B-Instruct** for best quality
2. Try **Qwen/Qwen2.5-14B-Instruct** for good balance
3. Try **microsoft/Phi-3-medium-4k-instruct** for reasoning

---

## üîó Quick Links

- **Server**: http://localhost:8080
- **HuggingFace Tokens**: See HPC/QUICK_START.md
- **Model Guide**: HPC/LOAD_MODELS_CORRECTLY.md
- **Upload Commands**: HPC/UPLOAD_COMMANDS.txt

---

**All ungated models listed above work without approval!** üéâ

